{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28941e7c-be2a-4e2a-83f3-86332284cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b98130c7-8444-4b49-93f8-db2166160dc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 23:49:17,668 - modelscope - INFO - PyTorch version 2.3.1 Found.\n",
      "2024-06-12 23:49:17,672 - modelscope - INFO - Loading ast index from /home/mila/s/soroush.omranpour/.cache/modelscope/ast_indexer\n",
      "2024-06-12 23:49:18,352 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 5eb56b46a2b5de5a5690e9f19ec1ca5d and a total number of 980 components indexed\n",
      "2024-06-12 23:49:19,967 - modelscope - WARNING - Using the master branch is fragile, please use it with caution!\n",
      "2024-06-12 23:49:19,969 - modelscope - INFO - Use user-specified model revision: master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detect model requirements, begin to install it: /home/mila/s/soroush.omranpour/.cache/modelscope/hub/iic/emotion2vec_plus_large/requirements.txt\n",
      "install model requirements successfully\n",
      "ckpt: /home/mila/s/soroush.omranpour/.cache/modelscope/hub/iic/emotion2vec_plus_large/model.pt\n",
      "init param, map: modality_encoders.AUDIO.extra_tokens from d2v_model.modality_encoders.AUDIO.extra_tokens in ckpt\n",
      "init param, map: modality_encoders.AUDIO.alibi_scale from d2v_model.modality_encoders.AUDIO.alibi_scale in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.0.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.1.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.2.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.3.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.4.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.5.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias from d2v_model.modality_encoders.AUDIO.local_encoder.conv_layers.6.2.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.weight from d2v_model.modality_encoders.AUDIO.project_features.1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.1.bias from d2v_model.modality_encoders.AUDIO.project_features.1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.weight from d2v_model.modality_encoders.AUDIO.project_features.2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.project_features.2.bias from d2v_model.modality_encoders.AUDIO.project_features.2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.1.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.1.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.2.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.2.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.3.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.3.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.4.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.4.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.weight from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.relative_positional_encoder.5.0.bias from d2v_model.modality_encoders.AUDIO.relative_positional_encoder.5.0.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.norm2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias from d2v_model.modality_encoders.AUDIO.context_encoder.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.weight from d2v_model.modality_encoders.AUDIO.context_encoder.norm.weight in ckpt\n",
      "init param, map: modality_encoders.AUDIO.context_encoder.norm.bias from d2v_model.modality_encoders.AUDIO.context_encoder.norm.bias in ckpt\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.0.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.0.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.1.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.1.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.2.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.2.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.weight, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.blocks.3.0.bias, mapped: modality_encoders.AUDIO.decoder.blocks.3.0.bias\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.weight, mapped: modality_encoders.AUDIO.decoder.proj.weight\n",
      "Warning, miss key in ckpt: modality_encoders.AUDIO.decoder.proj.bias, mapped: modality_encoders.AUDIO.decoder.proj.bias\n",
      "init param, map: blocks.0.norm1.weight from d2v_model.blocks.0.norm1.weight in ckpt\n",
      "init param, map: blocks.0.norm1.bias from d2v_model.blocks.0.norm1.bias in ckpt\n",
      "init param, map: blocks.0.attn.qkv.weight from d2v_model.blocks.0.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.0.attn.qkv.bias from d2v_model.blocks.0.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.0.attn.proj.weight from d2v_model.blocks.0.attn.proj.weight in ckpt\n",
      "init param, map: blocks.0.attn.proj.bias from d2v_model.blocks.0.attn.proj.bias in ckpt\n",
      "init param, map: blocks.0.norm2.weight from d2v_model.blocks.0.norm2.weight in ckpt\n",
      "init param, map: blocks.0.norm2.bias from d2v_model.blocks.0.norm2.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.weight from d2v_model.blocks.0.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc1.bias from d2v_model.blocks.0.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.weight from d2v_model.blocks.0.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.0.mlp.fc2.bias from d2v_model.blocks.0.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.1.norm1.weight from d2v_model.blocks.1.norm1.weight in ckpt\n",
      "init param, map: blocks.1.norm1.bias from d2v_model.blocks.1.norm1.bias in ckpt\n",
      "init param, map: blocks.1.attn.qkv.weight from d2v_model.blocks.1.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.1.attn.qkv.bias from d2v_model.blocks.1.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.1.attn.proj.weight from d2v_model.blocks.1.attn.proj.weight in ckpt\n",
      "init param, map: blocks.1.attn.proj.bias from d2v_model.blocks.1.attn.proj.bias in ckpt\n",
      "init param, map: blocks.1.norm2.weight from d2v_model.blocks.1.norm2.weight in ckpt\n",
      "init param, map: blocks.1.norm2.bias from d2v_model.blocks.1.norm2.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.weight from d2v_model.blocks.1.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc1.bias from d2v_model.blocks.1.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.weight from d2v_model.blocks.1.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.1.mlp.fc2.bias from d2v_model.blocks.1.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.2.norm1.weight from d2v_model.blocks.2.norm1.weight in ckpt\n",
      "init param, map: blocks.2.norm1.bias from d2v_model.blocks.2.norm1.bias in ckpt\n",
      "init param, map: blocks.2.attn.qkv.weight from d2v_model.blocks.2.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.2.attn.qkv.bias from d2v_model.blocks.2.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.2.attn.proj.weight from d2v_model.blocks.2.attn.proj.weight in ckpt\n",
      "init param, map: blocks.2.attn.proj.bias from d2v_model.blocks.2.attn.proj.bias in ckpt\n",
      "init param, map: blocks.2.norm2.weight from d2v_model.blocks.2.norm2.weight in ckpt\n",
      "init param, map: blocks.2.norm2.bias from d2v_model.blocks.2.norm2.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.weight from d2v_model.blocks.2.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc1.bias from d2v_model.blocks.2.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.weight from d2v_model.blocks.2.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.2.mlp.fc2.bias from d2v_model.blocks.2.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.3.norm1.weight from d2v_model.blocks.3.norm1.weight in ckpt\n",
      "init param, map: blocks.3.norm1.bias from d2v_model.blocks.3.norm1.bias in ckpt\n",
      "init param, map: blocks.3.attn.qkv.weight from d2v_model.blocks.3.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.3.attn.qkv.bias from d2v_model.blocks.3.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.3.attn.proj.weight from d2v_model.blocks.3.attn.proj.weight in ckpt\n",
      "init param, map: blocks.3.attn.proj.bias from d2v_model.blocks.3.attn.proj.bias in ckpt\n",
      "init param, map: blocks.3.norm2.weight from d2v_model.blocks.3.norm2.weight in ckpt\n",
      "init param, map: blocks.3.norm2.bias from d2v_model.blocks.3.norm2.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.weight from d2v_model.blocks.3.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc1.bias from d2v_model.blocks.3.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.weight from d2v_model.blocks.3.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.3.mlp.fc2.bias from d2v_model.blocks.3.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.4.norm1.weight from d2v_model.blocks.4.norm1.weight in ckpt\n",
      "init param, map: blocks.4.norm1.bias from d2v_model.blocks.4.norm1.bias in ckpt\n",
      "init param, map: blocks.4.attn.qkv.weight from d2v_model.blocks.4.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.4.attn.qkv.bias from d2v_model.blocks.4.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.4.attn.proj.weight from d2v_model.blocks.4.attn.proj.weight in ckpt\n",
      "init param, map: blocks.4.attn.proj.bias from d2v_model.blocks.4.attn.proj.bias in ckpt\n",
      "init param, map: blocks.4.norm2.weight from d2v_model.blocks.4.norm2.weight in ckpt\n",
      "init param, map: blocks.4.norm2.bias from d2v_model.blocks.4.norm2.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.weight from d2v_model.blocks.4.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc1.bias from d2v_model.blocks.4.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.weight from d2v_model.blocks.4.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.4.mlp.fc2.bias from d2v_model.blocks.4.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.5.norm1.weight from d2v_model.blocks.5.norm1.weight in ckpt\n",
      "init param, map: blocks.5.norm1.bias from d2v_model.blocks.5.norm1.bias in ckpt\n",
      "init param, map: blocks.5.attn.qkv.weight from d2v_model.blocks.5.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.5.attn.qkv.bias from d2v_model.blocks.5.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.5.attn.proj.weight from d2v_model.blocks.5.attn.proj.weight in ckpt\n",
      "init param, map: blocks.5.attn.proj.bias from d2v_model.blocks.5.attn.proj.bias in ckpt\n",
      "init param, map: blocks.5.norm2.weight from d2v_model.blocks.5.norm2.weight in ckpt\n",
      "init param, map: blocks.5.norm2.bias from d2v_model.blocks.5.norm2.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.weight from d2v_model.blocks.5.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc1.bias from d2v_model.blocks.5.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.weight from d2v_model.blocks.5.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.5.mlp.fc2.bias from d2v_model.blocks.5.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.6.norm1.weight from d2v_model.blocks.6.norm1.weight in ckpt\n",
      "init param, map: blocks.6.norm1.bias from d2v_model.blocks.6.norm1.bias in ckpt\n",
      "init param, map: blocks.6.attn.qkv.weight from d2v_model.blocks.6.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.6.attn.qkv.bias from d2v_model.blocks.6.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.6.attn.proj.weight from d2v_model.blocks.6.attn.proj.weight in ckpt\n",
      "init param, map: blocks.6.attn.proj.bias from d2v_model.blocks.6.attn.proj.bias in ckpt\n",
      "init param, map: blocks.6.norm2.weight from d2v_model.blocks.6.norm2.weight in ckpt\n",
      "init param, map: blocks.6.norm2.bias from d2v_model.blocks.6.norm2.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.weight from d2v_model.blocks.6.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc1.bias from d2v_model.blocks.6.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.weight from d2v_model.blocks.6.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.6.mlp.fc2.bias from d2v_model.blocks.6.mlp.fc2.bias in ckpt\n",
      "init param, map: blocks.7.norm1.weight from d2v_model.blocks.7.norm1.weight in ckpt\n",
      "init param, map: blocks.7.norm1.bias from d2v_model.blocks.7.norm1.bias in ckpt\n",
      "init param, map: blocks.7.attn.qkv.weight from d2v_model.blocks.7.attn.qkv.weight in ckpt\n",
      "init param, map: blocks.7.attn.qkv.bias from d2v_model.blocks.7.attn.qkv.bias in ckpt\n",
      "init param, map: blocks.7.attn.proj.weight from d2v_model.blocks.7.attn.proj.weight in ckpt\n",
      "init param, map: blocks.7.attn.proj.bias from d2v_model.blocks.7.attn.proj.bias in ckpt\n",
      "init param, map: blocks.7.norm2.weight from d2v_model.blocks.7.norm2.weight in ckpt\n",
      "init param, map: blocks.7.norm2.bias from d2v_model.blocks.7.norm2.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.weight from d2v_model.blocks.7.mlp.fc1.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc1.bias from d2v_model.blocks.7.mlp.fc1.bias in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.weight from d2v_model.blocks.7.mlp.fc2.weight in ckpt\n",
      "init param, map: blocks.7.mlp.fc2.bias from d2v_model.blocks.7.mlp.fc2.bias in ckpt\n"
     ]
    }
   ],
   "source": [
    "from funasr import AutoModel\n",
    "\n",
    "model = AutoModel(model=\"iic/emotion2vec_plus_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14dea527-e2dc-4df3-9ae8-a158818320fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n",
      "/home/mila/s/soroush.omranpour/Projects/lucid_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/mila/s/soroush.omranpour/Projects/lucid_env/lib/python3.10/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "speechbrain.lobes.models.huggingface_transformers.huggingface - Wav2Vec2Model is frozen.\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.inference.interfaces import foreign_class\n",
    "\n",
    "\n",
    "classifier = foreign_class(\n",
    "    source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", \n",
    "    pymodule_file=\"custom_interface.py\", \n",
    "    classname=\"CustomEncoderWav2vec2Classifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2388bd0-e863-43a5-9bba-3da96d148da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/s/soroush.omranpour/Projects/lucid_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/mila/s/soroush.omranpour/Projects/lucid_env/lib/python3.10/site-packages/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.output.bias', 'classifier.output.weight', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, AutoFeatureExtractor\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition'\n",
    ")\n",
    "hf_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c147360-ba89-4f9e-a790-c3f82b04ff00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14401202,), 16000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, fs = librosa.load('LANDR- Meditation- Transcend 15 Mins-Balanced-Medium.mp3', sr=16000)\n",
    "y.shape, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212872b4-5c1f-4687-9042-4b45a68d3daa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9d080cc7b0437f8437f2db9c00be90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "classes = ['angry', 'happy', 'neutral', 'sad', 'unknown']\n",
    "\n",
    "cache = {}\n",
    "n = y.shape[0]\n",
    "m = 5 * fs\n",
    "k = math.ceil(n / m)\n",
    "\n",
    "res = []\n",
    "for i in tqdm(range(k)):\n",
    "    # _, _, _, text_lab = classifier.classify_batch(torch.tensor(y[i*m:(i+1)*m]).unsqueeze(0))\n",
    "    # res += text_lab\n",
    "    \n",
    "    # scores = model.generate(\n",
    "    #     input=y[i*m:(i+1)*m], \n",
    "    #     is_final=(i == (k - 1)), \n",
    "    #     chunk_size=m,\n",
    "    #     cache=cache,\n",
    "    #     verbose=-1\n",
    "    # )[0]['scores']\n",
    "    # res += [classes[np.argmax(scores)]]\n",
    "\n",
    "    inputs = feature_extractor(y[i*m:(i+1)*m], sampling_rate=fs, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = hf_model(**inputs).logits\n",
    "        pred = torch.argmax(logits, dim=-1).item()\n",
    "        pred = hf_model.config.id2label[pred]\n",
    "        res += [pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebeda21b-3576-444f-9618-7af8bd4869de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'neutral',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'calm',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'neutral',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'calm',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e845056b-0344-4906-a399-78fee4796da8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
